# AI Dashboard Explanation - Endpoint V3 con Streaming (SSE)

## üìã Resumen

El endpoint **v3** utiliza **Server-Sent Events (SSE)** para enviar la respuesta de Gemini en tiempo real conforme se genera, mejorando significativamente la experiencia del usuario.

### Diferencias entre versiones

| Caracter√≠stica | V2 (Actual) | V3 (Nuevo - Streaming) |
|----------------|-------------|------------------------|
| Tiempo de espera | ~8 segundos completos | Respuesta inmediata (fragmentos) |
| M√©todo API Gemini | `generateContent` | `streamGenerateContent` |
| Tipo de respuesta | JSON √∫nico | Server-Sent Events (SSE) |
| UX | Usuario espera pantalla en blanco | Usuario ve texto aparecer en vivo |
| Endpoint | `POST /api/v2/ai/explain-dashboard` | `POST /api/v3/ai/explain-dashboard-stream` |

---

## üöÄ Uso del Endpoint V3

### Endpoint

```
POST /api/v3/ai/explain-dashboard-stream
Content-Type: application/json
```

### Request Body (igual que v2)

```json
{
  "dashboardId": 5,
  "fechaInicio": "2025-01-01",
  "fechaFin": "2025-01-31",
  "filtros": {}
}
```

### Response

El endpoint retorna un **stream de Server-Sent Events** con el siguiente formato:

```
event: message
data: {"resumenEjecutivo":"En el periodo analizado...

event: message  
data: se observa un crecimiento del 15%...

event: message
data: en las m√©tricas clave de producci√≥n...

event: done
data: [DONE]
```

---

## üíª Implementaci√≥n Frontend

### Opci√≥n 1: EventSource API (Recomendada)

**Limitaci√≥n**: EventSource solo soporta GET. Para POST con body, usar Opci√≥n 2.

### Opci√≥n 2: Fetch API con ReadableStream (Recomendada para POST)

```javascript
async function explainDashboardStreaming(dashboardId, fechaInicio, fechaFin) {
  const response = await fetch('/api/v3/ai/explain-dashboard-stream', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${token}`
    },
    body: JSON.stringify({
      dashboardId,
      fechaInicio,
      fechaFin,
      filtros: {}
    })
  });

  const reader = response.body.getReader();
  const decoder = new TextDecoder();
  let buffer = '';

  while (true) {
    const { done, value } = await reader.read();
    
    if (done) {
      console.log('Stream completed');
      break;
    }

    buffer += decoder.decode(value, { stream: true });
    
    // Procesar eventos SSE del buffer
    const lines = buffer.split('\n\n');
    buffer = lines.pop(); // Guardar fragmento incompleto
    
    for (const line of lines) {
      if (line.trim() === '') continue;
      
      const eventMatch = line.match(/^event: (.+)$/m);
      const dataMatch = line.match(/^data: (.+)$/m);
      
      if (eventMatch && dataMatch) {
        const event = eventMatch[1];
        const data = dataMatch[1];
        
        if (event === 'message') {
          // Agregar fragmento al UI
          appendTextToExplanation(data);
        } else if (event === 'done') {
          console.log('Explanation complete');
          break;
        } else if (event === 'error') {
          console.error('Error:', data);
        }
      }
    }
  }
}

function appendTextToExplanation(text) {
  const container = document.getElementById('explanation-container');
  container.textContent += text;
}
```

### Opci√≥n 3: Usando librer√≠a (m√°s simple)

```javascript
import { fetchEventSource } from '@microsoft/fetch-event-source';

await fetchEventSource('/api/v3/ai/explain-dashboard-stream', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': `Bearer ${token}`
  },
  body: JSON.stringify({
    dashboardId: 5,
    fechaInicio: '2025-01-01',
    fechaFin: '2025-01-31'
  }),
  onmessage(event) {
    if (event.event === 'message') {
      appendTextToExplanation(event.data);
    } else if (event.event === 'done') {
      console.log('Stream complete');
    }
  },
  onerror(err) {
    console.error('Stream error:', err);
    throw err;
  }
});
```

Instalaci√≥n:
```bash
npm install @microsoft/fetch-event-source
```

---

## üîß Componente React Ejemplo

```jsx
import React, { useState } from 'react';
import { fetchEventSource } from '@microsoft/fetch-event-source';

function DashboardExplanationStream({ dashboardId, fechaInicio, fechaFin }) {
  const [explanation, setExplanation] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState(null);

  const handleExplain = async () => {
    setIsLoading(true);
    setExplanation('');
    setError(null);

    try {
      await fetchEventSource('/api/v3/ai/explain-dashboard-stream', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${getToken()}`
        },
        body: JSON.stringify({
          dashboardId,
          fechaInicio,
          fechaFin,
          filtros: {}
        }),
        onmessage(event) {
          if (event.event === 'message') {
            setExplanation(prev => prev + event.data);
          } else if (event.event === 'done') {
            setIsLoading(false);
          }
        },
        onerror(err) {
          setError('Error al generar explicaci√≥n');
          setIsLoading(false);
          throw err;
        }
      });
    } catch (err) {
      setError(err.message);
      setIsLoading(false);
    }
  };

  return (
    <div>
      <button onClick={handleExplain} disabled={isLoading}>
        {isLoading ? 'Generando...' : 'Explicar Dashboard'}
      </button>
      
      {error && <div className="error">{error}</div>}
      
      <div className="explanation-container">
        {explanation}
        {isLoading && <span className="cursor">‚ñä</span>}
      </div>
    </div>
  );
}
```

---

## ‚öôÔ∏è Arquitectura Backend

### Flujo de Datos

```
Controller V3 (SSE)
    ‚Üì
StreamingDashboardExplanationService
    ‚Üì
1. Obtener metadata del dashboard (Metabase)
2. Construir prompt con datos
3. Llamar StreamingGeminiApiClient
    ‚Üì
Gemini API (streamGenerateContent)
    ‚Üì
Flux<String> ‚Üí Flux<ServerSentEvent<String>>
    ‚Üì
Frontend (recibe fragmentos en tiempo real)
```

### Clases Implementadas

1. **`AiExplanationControllerV3`**
   - Endpoint: `POST /api/v3/ai/explain-dashboard-stream`
   - Retorna: `Flux<ServerSentEvent<String>>`
   - Rate limiting: 5 requests/minuto (igual que v2)

2. **`StreamingDashboardExplanationService`**
   - Obtiene datos del dashboard
   - Construye el prompt
   - Coordina el streaming

3. **`StreamingGeminiApiClient`**
   - Llama a `streamGenerateContent` de Gemini
   - Parsea chunks JSON del stream
   - Retorna `Flux<String>` con fragmentos de texto

---

## üîë Configuraci√≥n

El endpoint v3 usa la misma configuraci√≥n que v2 en `application.properties`:

```properties
# Modelo Gemini (IMPORTANTE: usa el mismo modelo)
gemini.model=gemini-2.5-flash
gemini.api-key=${GEMINI_API_KEY}
gemini.base-url=https://generativelanguage.googleapis.com
gemini.timeout.seconds=90

# Rate Limiting
resilience4j.ratelimiter.instances.aiExplanation.limit-for-period=5
resilience4j.ratelimiter.instances.aiExplanation.limit-refresh-period=60s
```

---

## üß™ Pruebas

### Curl Test

```bash
curl -N -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_TOKEN" \
     -d '{
       "dashboardId": 5,
       "fechaInicio": "2025-01-01",
       "fechaFin": "2025-01-31"
     }' \
     http://localhost:8080/api/v3/ai/explain-dashboard-stream
```

La opci√≥n `-N` deshabilita el buffering para ver el stream en tiempo real.

### Test Browser (Console)

```javascript
const response = await fetch('/api/v3/ai/explain-dashboard-stream', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    dashboardId: 5,
    fechaInicio: '2025-01-01',
    fechaFin: '2025-01-31'
  })
});

const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const { done, value } = await reader.read();
  if (done) break;
  console.log(decoder.decode(value));
}
```

---

## üìä Ventajas del Streaming

1. **Mejor UX**: Usuario ve respuesta aparecer inmediatamente
2. **Percepci√≥n de velocidad**: Aunque el tiempo total sea similar, se siente m√°s r√°pido
3. **Feedback visual**: Usuario sabe que el sistema est√° procesando
4. **Manejo de timeouts**: Menos probabilidad de timeout percibido
5. **Cancelaci√≥n**: Frontend puede cancelar el stream si el usuario navega

---

## üîÑ Migraci√≥n de V2 a V3

### Mantener V2
- V2 seguir√° funcionando para clientes que prefieran respuesta completa
- √ötil para integraciones que no soporten SSE

### Estrategia de Migraci√≥n

1. **Fase 1**: Implementar v3 en paralelo (HECHO ‚úÖ)
2. **Fase 2**: Actualizar frontend para usar v3
3. **Fase 3**: A/B testing para comparar UX
4. **Fase 4**: Deprecar v2 si v3 funciona mejor

---

## üêõ Troubleshooting

### Error: "Stream is closed"
- **Causa**: Timeout del proxy/nginx
- **Soluci√≥n**: Aumentar timeout en proxy para endpoints SSE

### Error: "Cannot read property of null"
- **Causa**: Stream cerrado antes de completar
- **Soluci√≥n**: Implementar manejo de reconexi√≥n en frontend

### Chunks no llegan
- **Causa**: Buffering del proxy
- **Soluci√≥n**: Configurar nginx con `proxy_buffering off` para rutas SSE

---

## üìù Notas Importantes

1. **Gemini Model**: Aseg√∫rate de usar `gemini-2.5-flash` o superior (soporta streaming)
2. **CORS**: Si frontend est√° en otro dominio, configurar CORS para SSE
3. **Authentication**: El token debe enviarse en cada request (no se mantiene conexi√≥n persistente como WebSocket)
4. **Rate Limiting**: Aplica igual que v2 (5 requests/minuto por usuario)

---

## üéØ Pr√≥ximos Pasos

1. ‚úÖ Implementar endpoint v3 con streaming
2. ‚è≥ Actualizar frontend para consumir SSE
3. ‚è≥ Agregar m√©tricas de latencia por chunk
4. ‚è≥ Implementar tests de integraci√≥n para streaming
5. ‚è≥ Documentar en Swagger/OpenAPI

---

**Endpoint V3 creado y listo para usar** üöÄ

Archivos implementados:
- `AiExplanationControllerV3.java`
- `StreamingDashboardExplanationService.java`
- `StreamingGeminiApiClient.java`

